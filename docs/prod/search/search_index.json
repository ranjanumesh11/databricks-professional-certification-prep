{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"databricks-professional-certification-prep docs","text":""},{"location":"#exam-review-notes-welcome","title":"Exam Review Notes \u2014 Welcome","text":"<p>Welcome! This repository hosts concise study materials for the Databricks Professional Certification.</p> <ul> <li>Exam Review Notes: Open the published study notes (prod)</li> </ul> <p>If you want a staging/staging preview, view the <code>/dev/</code> site instead: Dev preview</p> <p>This landing page intentionally keeps content minimal and points contributors/readers to the published study notes.</p>"},{"location":"2-data-modeling/","title":"Section 2 \u2014 Data Modeling","text":"<p>Below are two concise reference sections followed by the detailed notebook explanations (theory and hands-on checklist). The first section is a high-level summary grouped by area (Databricks, Spark, Python, SQL). The second is a command/operator quick-reference table that shows syntax used in this course and a few useful extras marked with an asterisk (*).</p>"},{"location":"2-data-modeling/#1-high-level-concepts-by-area-short-bullets","title":"1) High-level concepts (by area) \u2014 short bullets","text":""},{"location":"2-data-modeling/#databricks-specific","title":"Databricks-specific","text":"<ul> <li>Autoloader (cloudFiles): incremental file ingestion for cloud storage (used to stream JSON files into Bronze). Doc: https://docs.databricks.com/data-engineering/ingestion/auto-loader/index.html</li> <li><code>availableNow</code> trigger: bounded processing of all currently-available files (demo/backfill). Doc: https://docs.databricks.com/data-engineering/ingestion/auto-loader/cloud-files-trigger.html</li> <li>Databricks Volumes / Unity Catalog: managed storage and cataloging for datasets. Doc: https://docs.databricks.com/data-governance/unity-catalog/index.html</li> <li><code>dbutils.fs</code>: workspace filesystem utilities (ls, cp, rm). Doc: https://docs.databricks.com/dev-tools/databricks-utils.html</li> </ul>"},{"location":"2-data-modeling/#spark-pyspark","title":"Spark / PySpark","text":"<ul> <li>Structured Streaming: <code>readStream</code> / <code>writeStream</code>, watermarking, triggers, <code>foreachBatch</code>. Docs: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</li> <li>DataFrame APIs: <code>from_json</code>, <code>withColumn</code>, <code>cast</code>, <code>date_format</code>, <code>dropDuplicates</code>, <code>withWatermark</code>. Docs: https://spark.apache.org/docs/latest/api/python/</li> <li>Window functions and broadcast joins for dedup/enrichment. Docs: https://spark.apache.org/docs/latest/api/python/</li> </ul>"},{"location":"2-data-modeling/#python","title":"Python","text":"<ul> <li>Core language constructs used: classes (<code>__init__</code>, <code>self</code>), lists/dicts/sets, f-strings, try/except, helper functions. Python stdlib reference: https://docs.python.org/3/library/stdtypes.html</li> </ul>"},{"location":"2-data-modeling/#sql-delta","title":"SQL / Delta","text":"<ul> <li>Delta Lake <code>MERGE</code> for upserts and Type-2 SCD patterns. Docs: https://docs.delta.io/latest/delta-update.html#merge</li> <li>Managed Delta tables via <code>.table(\"name\")</code> sink and <code>mergeSchema</code> option.</li> </ul>"},{"location":"2-data-modeling/#2-quick-commands-syntax-reference-grouped-by-area","title":"2) Quick commands &amp; syntax reference (grouped by area)","text":"<p>Notes: To improve readability this section is split into small tables per area (Databricks, PySpark, SQL/Delta, Python). Items are numbered and options or sub-settings appear as indented sub-items (e.g. <code>1.1</code>).</p> Databricks (1) \u2014 click to expand Item Command / Operator Course usage (short) Notes &amp; extras 1 \u2014 Autoloader <code>spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(path)</code> used to ingest <code>kafka-raw</code> -&gt; streaming DF Extra options listed below (1.1, 1.2) 1.1 \u2014 Autoloader option <code>.option(\"cloudFiles.format\",\"json\")</code> specify payload format common: <code>json</code>, <code>parquet</code>, <code>csv</code> 1.2 \u2014 Autoloader notifications* <code>.option(\"cloudFiles.useNotifications\",\"true\")</code> use S3 notifications to lower list cost *Requires configuration of cloud events 2 \u2014 Trigger <code>.trigger(availableNow=True)</code> used in <code>process_bronze()</code> to process available files and stop Alternative: <code>.trigger(processingTime='10 seconds')</code> 3 \u2014 Checkpointing <code>.option(\"checkpointLocation\", checkpoint_path)</code> required on writeStream for progress tracking ensure stable storage; use unique path per query 4 \u2014 Partitioning (write) <code>.partitionBy(\"topic\",\"year_month\")</code> write partitioned Delta table partitions improve read performance; avoid small partitions 5 \u2014 Delta write options <code>.option(\"mergeSchema\", True).table(\"bronze\")</code> allow schema evolution on sink Extra: <code>.mode(\"append\")</code>, <code>format(\"delta\")</code> 6 \u2014 dbutils helpers <code>dbutils.fs.ls(path)</code>, <code>dbutils.fs.cp(src,dst)</code>, <code>dbutils.fs.rm(path, True)</code> copy and list dataset files (Copy-Datasets uses these) DBFS vs <code>/Volumes/...</code> differences by catalog Spark / PySpark (2) \u2014 click to expand Item Command / Operator Course usage (short) Notes &amp; extras 2.1 \u2014 Streaming read <code>spark.readStream.table(\"bronze\")</code> read Bronze as streaming source can also use <code>format(\"delta\")</code> + <code>.load(path)</code> 2.2 \u2014 JSON parsing <code>F.from_json(F.col(\"value\").cast(\"string\"), json_schema)</code> parse message payloads into struct <code>from_json</code> returns Struct; use <code>.select(\"v.*\")</code> to expand 2.3 \u2014 Timestamp handling <code>.withColumn(\"timestamp\", (F.col(\"timestamp\")/1000).cast(\"timestamp\"))</code> convert epoch ms -&gt; timestamp or use <code>to_timestamp</code> on string values 2.4 \u2014 Partition key <code>F.date_format(\"timestamp\",\"yyyy-MM\")</code> derive <code>year_month</code> partition column common for time-partitioning 2.5 \u2014 Watermarking <code>.withWatermark(\"order_timestamp\",\"30 seconds\")</code> used for dedup state cleanup choose watermark based on expected lateness 2.6 \u2014 Deduplication <code>.dropDuplicates([\"order_id\",\"order_timestamp\"])</code> remove duplicates in stream (with watermark) <code>dropDuplicates</code> is stateful; watch memory 2.7 \u2014 foreachBatch <code>.writeStream.foreachBatch(func).option(\"checkpointLocation\", ...).start()</code> used to call upsert functions per micro-batch <code>func(microBatchDF, batchId)</code> \u2014 use <code>MERGE</code> inside func 2.8 \u2014 Window &amp; rank <code>Window.partitionBy(...).orderBy(F.col(...).desc())</code> and <code>F.rank()</code> used in <code>upsert_customers_batch</code> for latest row selection Window functions are powerful for dedupe &amp; SCD logic 2.9 \u2014 Broadcast join <code>F.broadcast(df_small)</code> used to enrich customers with country lookup use for small static tables to avoid shuffle 2.10 \u2014 Spark actions <code>df.collect()</code> / <code>df.first()</code> / <code>df.take(n)</code> used to read small results to driver Prefer <code>first()</code> / <code>take(1)</code> over <code>collect()</code> for single-row reads SQL / Delta (3) \u2014 click to expand Item Command / Operator Course usage (short) Notes &amp; extras 3.1 \u2014 MERGE (Delta) <code>MERGE INTO target USING source ON &lt;cond&gt; WHEN MATCHED THEN UPDATE ... WHEN NOT MATCHED THEN INSERT ...</code> used for upserts &amp; Type-2 SCD Delta support for <code>MERGE</code> SQL; essential for exam Python (4) \u2014 click to expand Item Command / Operator Course usage (short) Notes &amp; extras 4.1 \u2014 Basics <code>class</code>, <code>__init__</code>, <code>self</code>, <code>list.append()</code>, <code>dict</code> <code>Copy-Datasets</code> creates <code>CourseDataset</code> instance and calls methods <code>__init__</code> initializes attributes; avoid heavy I/O in constructor"},{"location":"2-data-modeling/#short-autoloader-example","title":"Short Autoloader example","text":"<pre><code># Autoloader: read JSON files incrementally from cloud storage\nautoloader_df = (spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"json\")\n    .schema(\"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\")\n    .load(f\"{bookstore.dataset_path}/kafka-raw\")\n)\n\n# Cast value to string and parse later with from_json in downstream processing\nautoloader_df.select(\"topic\", autoloader_df.value.cast(\"string\").alias(\"value_text\")).show(5)\n</code></pre>"},{"location":"2-data-modeling/#3-where-to-read-official-docs","title":"3) Where to read (official docs)","text":"<ul> <li>Databricks Autoloader: https://docs.databricks.com/data-engineering/ingestion/auto-loader/index.html</li> <li>Databricks <code>dbutils</code>: https://docs.databricks.com/dev-tools/databricks-utils.html</li> <li>Databricks Unity Catalog &amp; Volumes: https://docs.databricks.com/data-governance/unity-catalog/index.html</li> <li>Spark Structured Streaming guide: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</li> <li>PySpark functions (from_json, date_format, etc): https://spark.apache.org/docs/latest/api/python/</li> <li>Delta Lake MERGE &amp; update patterns: https://docs.delta.io/latest/delta-update.html#merge</li> <li>Python stdlib (core types): https://docs.python.org/3/library/stdtypes.html</li> </ul>"}]}