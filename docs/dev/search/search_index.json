{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Exam Review Notes","text":"<p>Welcome! This repository hosts concise study materials for the Databricks Professional Certification.</p>"},{"location":"2-data-modeling/","title":"Section 2 \u2014 Data Modeling","text":"<p>Below are two concise reference sections followed by the detailed notebook explanations (theory and hands-on checklist). The first section is a high-level summary grouped by area (Databricks, Spark, Python, SQL). The second is a command/operator quick-reference table that shows syntax used in this course and a few useful extras marked with an asterisk (*).</p>"},{"location":"2-data-modeling/#1-high-level-concepts-by-area-short-bullets","title":"1) High-level concepts (by area) \u2014 short bullets","text":""},{"location":"2-data-modeling/#databricks-specific","title":"Databricks-specific","text":"<ul> <li>Autoloader (cloudFiles): incremental file ingestion for cloud storage (used to stream JSON files into Bronze). Doc</li> <li><code>availableNow</code> trigger: bounded processing of all currently-available files (demo/backfill). Doc</li> <li>Databricks Volumes / Unity Catalog: managed storage and cataloging for datasets. Doc</li> <li><code>dbutils.fs</code>: workspace filesystem utilities (ls, cp, rm). Doc</li> </ul>"},{"location":"2-data-modeling/#spark-pyspark","title":"Spark / PySpark","text":"<ul> <li>Structured Streaming: <code>readStream</code> / <code>writeStream</code>, watermarking, triggers, <code>foreachBatch</code>. Docs</li> <li>DataFrame APIs: <code>from_json</code>, <code>withColumn</code>, <code>cast</code>, <code>date_format</code>, <code>dropDuplicates</code>, <code>withWatermark</code>. Docs</li> <li>Window functions and broadcast joins for dedup/enrichment. Docs</li> </ul>"},{"location":"2-data-modeling/#python","title":"Python","text":"<ul> <li>Core language constructs used: classes (<code>__init__</code>, <code>self</code>), lists/dicts/sets, f-strings, try/except, helper functions. Python stdlib reference</li> </ul>"},{"location":"2-data-modeling/#sql-delta","title":"SQL / Delta","text":"<ul> <li>Delta Lake <code>MERGE</code> for upserts and Type-2 SCD patterns. Docs</li> <li>Managed Delta tables via <code>.table(\"name\")</code> sink and <code>mergeSchema</code> option.</li> </ul>"},{"location":"2-data-modeling/#2-quick-commands-syntax-reference-grouped-by-area","title":"2) Quick commands &amp; syntax reference (grouped by area)","text":"<p>Notes: To improve readability this section is split into small tables per area (Databricks, PySpark, SQL/Delta, Python). Items are numbered and options or sub-settings appear as indented sub-items (e.g. <code>1.1</code>).</p> Databricks (1) \u2014 click to expand Item Command / Operator Course usage (short) Notes &amp; extras 1 \u2014 Autoloader <code>spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(path)</code> used to ingest <code>kafka-raw</code> -&gt; streaming DF Extra options listed below (1.1, 1.2) 1.1 \u2014 Autoloader option <code>.option(\"cloudFiles.format\",\"json\")</code> specify payload format common: <code>json</code>, <code>parquet</code>, <code>csv</code> 1.2 \u2014 Autoloader notifications* <code>.option(\"cloudFiles.useNotifications\",\"true\")</code> use S3 notifications to lower list cost *Requires configuration of cloud events 2 \u2014 Trigger <code>.trigger(availableNow=True)</code> used in <code>process_bronze()</code> to process available files and stop Alternative: <code>.trigger(processingTime='10 seconds')</code> 3 \u2014 Checkpointing <code>.option(\"checkpointLocation\", checkpoint_path)</code> required on writeStream for progress tracking ensure stable storage; use unique path per query 4 \u2014 Partitioning (write) <code>.partitionBy(\"topic\",\"year_month\")</code> write partitioned Delta table partitions improve read performance; avoid small partitions 5 \u2014 Delta write options <code>.option(\"mergeSchema\", True).table(\"bronze\")</code> allow schema evolution on sink Extra: <code>.mode(\"append\")</code>, <code>format(\"delta\")</code> 6 \u2014 dbutils helpers <code>dbutils.fs.ls(path)</code>, <code>dbutils.fs.cp(src,dst)</code>, <code>dbutils.fs.rm(path, True)</code> copy and list dataset files (Copy-Datasets uses these) DBFS vs <code>/Volumes/...</code> differences by catalog Spark / PySpark (2) \u2014 click to expand Item Command / Operator Course usage (short) Notes &amp; extras 2.1 \u2014 Streaming read <code>spark.readStream.table(\"bronze\")</code> read Bronze as streaming source can also use <code>format(\"delta\")</code> + <code>.load(path)</code> 2.2 \u2014 JSON parsing <code>F.from_json(F.col(\"value\").cast(\"string\"), json_schema)</code> parse message payloads into struct <code>from_json</code> returns Struct; use <code>.select(\"v.*\")</code> to expand 2.3 \u2014 Timestamp handling <code>.withColumn(\"timestamp\", (F.col(\"timestamp\")/1000).cast(\"timestamp\"))</code> convert epoch ms -&gt; timestamp or use <code>to_timestamp</code> on string values 2.4 \u2014 Partition key <code>F.date_format(\"timestamp\",\"yyyy-MM\")</code> derive <code>year_month</code> partition column common for time-partitioning 2.5 \u2014 Watermarking <code>.withWatermark(\"order_timestamp\",\"30 seconds\")</code> used for dedup state cleanup choose watermark based on expected lateness 2.6 \u2014 Deduplication <code>.dropDuplicates([\"order_id\",\"order_timestamp\"])</code> remove duplicates in stream (with watermark) <code>dropDuplicates</code> is stateful; watch memory 2.7 \u2014 foreachBatch <code>.writeStream.foreachBatch(func).option(\"checkpointLocation\", ...).start()</code> used to call upsert functions per micro-batch <code>func(microBatchDF, batchId)</code> \u2014 use <code>MERGE</code> inside func 2.8 \u2014 Window &amp; rank <code>Window.partitionBy(...).orderBy(F.col(...).desc())</code> and <code>F.rank()</code> used in <code>upsert_customers_batch</code> for latest row selection Window functions are powerful for dedupe &amp; SCD logic 2.9 \u2014 Broadcast join <code>F.broadcast(df_small)</code> used to enrich customers with country lookup use for small static tables to avoid shuffle 2.10 \u2014 Spark actions <code>df.collect()</code> / <code>df.first()</code> / <code>df.take(n)</code> used to read small results to driver Prefer <code>first()</code> / <code>take(1)</code> over <code>collect()</code> for single-row reads SQL / Delta (3) \u2014 click to expand Item Command / Operator Course usage (short) Notes &amp; extras 3.1 \u2014 MERGE (Delta) <code>MERGE INTO target USING source ON &lt;cond&gt; WHEN MATCHED THEN UPDATE ... WHEN NOT MATCHED THEN INSERT ...</code> used for upserts &amp; Type-2 SCD Delta support for <code>MERGE</code> SQL; essential for exam Python (4) \u2014 click to expand Item Command / Operator Course usage (short) Notes &amp; extras 4.1 \u2014 Basics <code>class</code>, <code>__init__</code>, <code>self</code>, <code>list.append()</code>, <code>dict</code> <code>Copy-Datasets</code> creates <code>CourseDataset</code> instance and calls methods <code>__init__</code> initializes attributes; avoid heavy I/O in constructor"},{"location":"2-data-modeling/#short-autoloader-example","title":"Short Autoloader example","text":"<pre><code># Autoloader: read JSON files incrementally from cloud storage\nautoloader_df = (spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"json\")\n    .schema(\"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\")\n    .load(f\"{bookstore.dataset_path}/kafka-raw\")\n)\n\n# Cast value to string and parse later with from_json in downstream processing\nautoloader_df.select(\"topic\", autoloader_df.value.cast(\"string\").alias(\"value_text\")).show(5)\n</code></pre>"},{"location":"2-data-modeling/#3-where-to-read-official-docs","title":"3) Where to read (official docs)","text":"<ul> <li>Databricks Autoloader</li> <li>Databricks dbutils</li> <li>Databricks Unity Catalog &amp; Volumes</li> <li>Spark Structured Streaming guide</li> <li>PySpark functions (from_json, date_format, etc)</li> <li>Delta Lake MERGE &amp; update patterns</li> <li>Python stdlib (core types)</li> </ul>"},{"location":"3-data-processing/","title":"Section 3 \u2014 Data Processing","text":"<p>Quick-reference tables for streaming data processing, CDC, and joins.</p>"},{"location":"3-data-processing/#databricks-streaming-cdc","title":"Databricks Streaming &amp; CDC","text":"Change Data Capture (CDC) Operations Operation Code/Command Usage Notes Enable CDF on Table <code>ALTER TABLE &lt;table&gt; SET TBLPROPERTIES (delta.enableChangeDataFeed = true)</code> Enable Change Data Feed tracking Required for CDC operations Query Table Changes <code>SELECT * FROM table_changes('&lt;table&gt;', &lt;version&gt;)</code> Get changes since version Returns insert/update/delete operations Stream with CDF <code>.option(\"readChangeData\", True).option(\"startingVersion\", n)</code> Read change data as stream Use with readStream Filter Row Status <code>.filter(F.col(\"row_status\").isin([\"insert\", \"update\"]))</code> Process specific operations Common in CDC patterns Window Ranking <code>Window.partitionBy(\"id\").orderBy(F.col(\"time\").desc())</code> Get latest records Use with <code>F.rank().over(window)</code> Batch Upsert Pattern <code>foreachBatch(batch_upsert)</code> Custom micro-batch logic Combine window ops with MERGE Stream Processing Patterns Pattern Code/Command Usage Notes Stream-Stream Join <code>stream_df1.join(stream_df2, \"key\", \"inner\")</code> Join two streaming DataFrames Requires watermark for state management Stream-Static Join <code>stream_df.join(F.broadcast(static_df), \"key\")</code> Join stream with static lookup Use broadcast for small static tables Watermark <code>.withWatermark(\"timestamp_col\", \"10 minutes\")</code> Define late data threshold Required for stateful operations availableNow Trigger <code>.trigger(availableNow=True)</code> Process all available data once Micro-batch mode for incremental loads Checkpoint Location <code>.option(\"checkpointLocation\", path)</code> Track stream progress Required for fault tolerance foreachBatch <code>.writeStream.foreachBatch(func)</code> Custom batch processing Access to full DataFrame API per batch MERGE Operations Operation SQL Syntax Usage Notes Basic MERGE <code>MERGE INTO target USING source ON key WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT *</code> Upsert pattern Standard CDC implementation Conditional Update <code>WHEN MATCHED AND target.time &lt; source.time THEN UPDATE SET *</code> Update only newer records Prevents overwriting with old data Delete on Match <code>WHEN MATCHED THEN DELETE</code> Remove matched records Used for propagating deletes Multiple Conditions Multiple <code>WHEN MATCHED</code> clauses Complex merge logic Evaluated in order"},{"location":"3-data-processing/#pyspark-data-processing","title":"PySpark Data Processing","text":"DataFrame Transformations Operation Code/Command Usage Notes Parse JSON <code>F.from_json(F.col(\"value\").cast(\"string\"), schema)</code> Extract nested JSON Requires schema definition Select Nested <code>.select(\"v.*\")</code> Flatten struct columns After from_json Window Function <code>.withColumn(\"rank\", F.rank().over(window))</code> Ranking within partitions Not supported in streaming without foreachBatch Broadcast Join <code>F.broadcast(small_df)</code> Optimize small table joins Use for lookup tables Filter Multiple <code>.filter(F.col(\"status\").isin([\"a\", \"b\"]))</code> Multiple value filter Alternative to OR conditions Date Operations <code>F.date_add(\"timestamp\", 30)</code> Date arithmetic Also date_sub, months_between, etc. Gold Table Aggregations Operation Code/Command Usage Notes Group &amp; Aggregate <code>.groupBy(\"country\").agg(F.count(\"*\").alias(\"total\"))</code> Basic aggregation Use multiple agg functions in one call Window Aggregate <code>.withColumn(\"running_total\", F.sum(\"amount\").over(window))</code> Running calculations Window without bounds = all rows in partition Pivot <code>.groupBy(\"country\").pivot(\"category\").agg(F.sum(\"sales\"))</code> Wide format transformation Creates column per pivot value Explode Array <code>.select(\"*\", F.explode(\"array_col\").alias(\"item\"))</code> Array to rows Creates new row for each array element"},{"location":"3-data-processing/#key-concepts","title":"Key Concepts","text":"<ol> <li>Change Data Capture (CDC): Track insert/update/delete operations on Delta tables using Change Data Feed</li> <li>Stream-Stream Joins: Require watermarks and proper windowing for state management</li> <li>foreachBatch Pattern: Enables full DataFrame API (including window functions) on streaming data by processing micro-batches</li> <li>MERGE Statement: Standard pattern for upserts; supports conditional logic for complex CDC scenarios</li> <li>Broadcast Joins: Optimize stream-static joins by broadcasting small lookup tables to all workers</li> </ol>"},{"location":"3-data-processing/#official-documentation","title":"Official Documentation","text":"<ul> <li>Delta Lake Change Data Feed</li> <li>Structured Streaming Programming Guide</li> <li>Delta Lake MERGE Operations</li> <li>Spark Structured Streaming + Kafka Integration</li> <li>PySpark Streaming Functions</li> </ul>"},{"location":"4-improving-performance/","title":"Section 4 \u2014 Improving Performance","text":"<p>Quick-reference tables for Python UDFs, vectorization, and performance optimization.</p>"},{"location":"4-improving-performance/#python-udfs","title":"Python UDFs","text":"UDF Types and Registration UDF Type Code/Command Usage Notes Standard UDF <code>udf(func)</code> or <code>spark.udf.register(\"name\", func)</code> Python UDF (row-by-row) Slow due to serialization overhead Decorator UDF <code>@udf(\"returnType\")</code> Define UDF with decorator Type must be specified as string Pandas UDF (Vectorized) <code>@pandas_udf(\"returnType\")</code> Vectorized operations Much faster than standard UDFs Register for SQL <code>spark.udf.register(\"sql_name\", udf_func)</code> Use UDF in SQL queries Available in <code>%sql</code> cells UDF Performance Patterns Pattern Code/Command Usage Notes Standard Python UDF <code>def func(x): return x * 2</code> then <code>udf(func)</code> Simple row-by-row logic Serializes data to Python process Pandas UDF Series <code>def func(s: pd.Series) -&gt; pd.Series: return s * 2</code> Vectorized column operations Processes batches with Pandas Apply UDF <code>df.select(my_udf(col(\"price\"), lit(50)))</code> Use in DataFrame API Can combine with <code>lit()</code> for constants SQL UDF Usage <code>SELECT my_udf(price, 50) FROM table</code> Use in SQL context Must register with <code>spark.udf.register()</code> UDF Best Practices Practice Code/Command Usage Notes Prefer Built-in Functions Use <code>F.when()</code>, <code>F.coalesce()</code>, etc. Native Spark functions Always faster than UDFs Use Pandas UDF <code>@pandas_udf(\"double\")</code> When UDF is necessary 10-100x faster than standard UDF Type Hints <code>def func(x: pd.Series) -&gt; pd.Series</code> Pandas UDF type safety Required for Pandas UDFs Avoid Complex Logic Keep UDF simple Minimize computation Consider breaking into steps Test Performance Compare with <code>.explain()</code> and timing Measure impact Standard UDFs create extra stages"},{"location":"4-improving-performance/#vectorization-with-pandas","title":"Vectorization with Pandas","text":"Pandas UDF Patterns Pattern Code/Command Usage Notes Scalar Pandas UDF <code>@pandas_udf(\"double\") def f(s: pd.Series) -&gt; pd.Series</code> Column transformation Most common pattern Grouped Map <code>@pandas_udf(..., functionType=PandasUDFType.GROUPED_MAP)</code> Group-level operations Deprecated in Spark 3.0+ Apply in Pandas <code>df.groupBy(\"key\").applyInPandas(func, schema)</code> Custom group processing Modern alternative to GROUPED_MAP Map Iterator <code>df.mapInPandas(func, schema)</code> Process batches Low memory overhead for large datasets Performance Optimization Technique Code/Command Usage Notes Cache <code>df.cache()</code> or <code>df.persist()</code> Reuse DataFrame Use for iterative operations Broadcast <code>F.broadcast(small_df)</code> Small table joins Avoid shuffle for small tables Repartition <code>df.repartition(n, \"key\")</code> Control parallelism Use before expensive operations Coalesce <code>df.coalesce(n)</code> Reduce partitions Minimize output files Predicate Pushdown <code>.filter()</code> early in chain Filter before joins Let Delta Lake skip files Column Pruning <code>.select()</code> only needed columns Reduce data volume Especially important with wide tables"},{"location":"4-improving-performance/#delta-lake-optimization","title":"Delta Lake Optimization","text":"Table Partitioning Concept Syntax/Command Usage Notes Create Partitioned Table <code>CREATE TABLE table PARTITIONED BY (col1, col2)</code> Physical separation into subdirectories Use for very large tables with low-cardinality columns Partition Columns Common: date/time, region, category Choose based on query filters Avoid high-cardinality columns (user_id, transaction_id) Directory Structure <code>table_location/col1=value1/col2=value2/</code> Hierarchical folder layout Enables partition-level access control Partition Pruning <code>WHERE date = '2023-01-01'</code> Skip entire partitions Query optimizer uses partition filters Trade-offs Small files per partition, rigid schema Not ideal for small/medium tables OPTIMIZE only runs per partition Z-Order Indexing Concept Syntax/Command Usage Notes Enable Z-Ordering <code>OPTIMIZE table ZORDER BY (col1, col2)</code> Co-locate column data in files No subdirectories created Data Co-location Groups similar values together Example: ID 1-50 in file1, 51-100 in file2 Improves data skipping efficiency Multi-column <code>ZORDER BY (customer_id, order_date)</code> Optimize for multiple filters Order matters; choose most selective first Non-incremental Must rerun on new data Full table reorganization Can be expensive for frequent updates File Statistics Min/max values per file Query planner skips irrelevant files Leveraged automatically by Delta Lake Liquid Clustering Concept Syntax/Command Usage Notes Create Clustered Table <code>CREATE TABLE table CLUSTER BY (col1, col2)</code> Define clustering at table level Modern replacement for Z-order Alter Existing Table <code>ALTER TABLE table CLUSTER BY (col1, col2)</code> Add clustering to existing table No data rewrite required Trigger Clustering <code>OPTIMIZE table</code> Compact and cluster files Incremental operation; only processes new data Redefine Keys <code>ALTER TABLE table CLUSTER BY (new_cols)</code> Change clustering columns Adapts to evolving query patterns Automatic Clustering <code>CREATE TABLE table CLUSTER BY AUTO</code> Databricks chooses keys Requires Predictive Optimization enabled Advantages Incremental, flexible, efficient Smart optimization Only rewrites unoptimized files Key Selection Based on query filters Analyze <code>WHERE</code> clause patterns Use frequently filtered columns Transaction Log &amp; Checkpoints Concept Details Usage Notes Transaction Log <code>_delta_log/</code> directory with JSON files Records all table operations Each commit = one JSON file Checkpoint Files Parquet files every 10 commits Snapshot of entire table state Faster than reading many JSONs File Statistics Min, max, nullCount per column Captured per data file Limited to first 32 columns Log Retention Default: 30 days Controls time travel window Set via <code>delta.logRetentionDuration</code> Statistics Columns <code>numRecords</code>, <code>minValues</code>, <code>maxValues</code> Per-file metadata Used for data skipping Data Skipping Automatic file pruning Query optimizer uses statistics No configuration needed Nested Fields Count toward 32-column limit Struct with 8 fields = 8 columns Order schema for important columns first Auto Optimize Feature Configuration Usage Notes Optimized Writes <code>delta.autoOptimize.optimizeWrite = true</code> Write 128MB files per partition Happens during write operation Auto Compaction <code>delta.autoOptimize.autoCompact = true</code> Compact after write completes 128MB target (not 1GB like manual OPTIMIZE) Table Properties Set on <code>CREATE TABLE</code> or <code>ALTER TABLE</code> Enable per table Can override per session Session Config <code>spark.databricks.delta.optimizeWrite.enabled</code> Enable for session Takes precedence over table properties Auto Tuning Adjusts file size for merge patterns Smaller files for frequent merges Reduces merge operation duration Z-Order Limitation Auto compaction doesn't support Z-order Use manual OPTIMIZE for Z-order Z-ordering is more expensive When to Use Streaming writes, frequent small batches Reduces small file problem Trade-off: slower writes for better reads Manual Optimization Commands Command Syntax Usage Notes Basic OPTIMIZE <code>OPTIMIZE table</code> Compact small files Target: 1GB files OPTIMIZE with Z-Order <code>OPTIMIZE table ZORDER BY (col1, col2)</code> Compact + co-locate data Computationally expensive VACUUM <code>VACUUM table [RETAIN hours]</code> Delete old data files Default: 7 days retention DESCRIBE HISTORY <code>DESCRIBE HISTORY table</code> View transaction log Shows commits, operations, versions DESCRIBE DETAIL <code>DESCRIBE DETAIL table</code> Table metadata Location, format, partitioning ANALYZE TABLE <code>ANALYZE TABLE table COMPUTE STATISTICS</code> Update table statistics Usually automatic in Delta Lake"},{"location":"4-improving-performance/#key-concepts","title":"Key Concepts","text":"<ol> <li>Standard UDFs: Row-by-row processing with high serialization overhead; avoid when possible</li> <li>Pandas UDFs: Vectorized processing using Apache Arrow for fast data transfer; 10-100x faster than standard UDFs</li> <li>Built-in Functions: Always prefer native Spark functions (<code>pyspark.sql.functions</code>) over UDFs</li> <li>Type Safety: Pandas UDFs require explicit type hints and return type specifications</li> <li>Registration: UDFs must be registered with <code>spark.udf.register()</code> to be used in SQL</li> <li>Performance: Minimize data movement, use broadcast joins, filter early, and select only needed columns</li> <li>Partitioning: Use for very large tables with low-cardinality partition columns; creates subdirectories</li> <li>Z-Order Indexing: Co-locates data without subdirectories; non-incremental, requires full rerun</li> <li>Liquid Clustering: Modern, incremental, flexible clustering; adapts to query patterns</li> <li>Transaction Log: JSON files record operations; checkpoint every 10 commits for fast state resolution</li> <li>File Statistics: Min/max/nullCount per column enables automatic data skipping</li> <li>Auto Optimize: Optimized writes (128MB files) + auto compaction; reduces small file problem</li> </ol>"},{"location":"4-improving-performance/#official-documentation","title":"Official Documentation","text":""},{"location":"4-improving-performance/#udfs-vectorization","title":"UDFs &amp; Vectorization","text":"<ul> <li>Pandas UDFs (Vectorized UDFs)</li> <li>PySpark SQL Functions</li> <li>Apache Arrow and Spark</li> <li>Spark Performance Tuning Guide</li> <li>UDF Best Practices</li> </ul>"},{"location":"4-improving-performance/#delta-lake-optimization_1","title":"Delta Lake Optimization","text":"<ul> <li>Delta Lake Optimize</li> <li>Z-Order Clustering</li> <li>Liquid Clustering</li> <li>Auto Optimize</li> <li>Delta Lake Transaction Log</li> <li>Predictive Optimization</li> <li>Table Partitioning</li> </ul>"},{"location":"5-data-orchestration/","title":"Section 5 \u2014 Data Orchestration","text":"<p>Quick-reference tables for multi-task jobs, task dependencies, and workflow orchestration.</p>"},{"location":"5-data-orchestration/#databricks-jobs-workflows","title":"Databricks Jobs &amp; Workflows","text":"Multi-Task Job Configuration Component Configuration Usage Notes Task Types Notebook, Python script, JAR, SQL, dbt Different execution types Mix in single job Task Dependencies <code>depends_on: [task1, task2]</code> Control execution order Tasks run in parallel when possible Job Parameters <code>base_parameters</code> at job level Pass to all tasks Task parameters override job parameters Task Parameters Individual task parameters Override job defaults Access via <code>dbutils.widgets.get()</code> Cluster Configuration Per-task or shared cluster Resource allocation Shared clusters reduce startup time Retry Policy <code>max_retries: 3</code>, <code>timeout_seconds: 3600</code> Handle failures Set per task Task Orchestration Patterns Pattern Configuration Usage Notes Sequential Tasks Linear <code>depends_on</code> chain ETL pipeline stages Task2 depends on Task1, Task3 on Task2, etc. Parallel Tasks No dependencies between tasks Independent processing Execute simultaneously Fan-out/Fan-in Multiple tasks depend on one, then converge Parallel processing + aggregation Common in data pipelines Conditional Execution Use <code>if-else</code> task type Dynamic workflows Skip tasks based on conditions Trigger Types Scheduled, file arrival, manual When to run Use file arrival for event-driven pipelines Parameter Passing &amp; Widgets Operation Code/Command Usage Notes Create Widget <code>dbutils.widgets.text(\"name\", \"default\")</code> Define input parameter Available in notebook UI Get Widget Value <code>dbutils.widgets.get(\"name\")</code> Access parameter value Returns string Remove Widget <code>dbutils.widgets.remove(\"name\")</code> Clean up Good practice at end of notebook Pass to Next Task <code>dbutils.notebook.exit(json.dumps(result))</code> Return value from notebook Use <code>taskValues</code> in downstream tasks Access Task Value <code>dbutils.jobs.taskValues.get(\"task_name\", \"key\")</code> Get value from previous task Only in multi-task jobs"},{"location":"5-data-orchestration/#workflow-monitoring-control","title":"Workflow Monitoring &amp; Control","text":"Job Monitoring Operation API/UI Usage Notes Job Runs Jobs UI &gt; Run History View execution history Filter by status, date Task Output Task details &gt; Output See returned values From <code>dbutils.notebook.exit()</code> Logs Task &gt; View Logs Troubleshoot failures Stdout, stderr, driver logs Metrics Task &gt; Metrics Performance monitoring Duration, data processed Email Alerts Job settings &gt; Alerts Failure notifications Can configure per task or job Webhooks Job settings &gt; Webhooks External integrations Trigger external systems Job Control &amp; Management Operation Code/Command Usage Notes Run Now UI or <code>runs submit</code> API Manual trigger Can override parameters Cancel Run UI or <code>runs cancel</code> API Stop execution Terminates all running tasks Repair Run UI &gt; Repair Re-run failed tasks Preserves successful task results Jobs API <code>databricks jobs create/update</code> Programmatic management Use for CI/CD dbutils.notebook.run <code>dbutils.notebook.run(\"path\", timeout, params)</code> Run nested notebook Synchronous execution %run <code>%run ./helper_notebook</code> Include notebook Shares namespace with calling notebook"},{"location":"5-data-orchestration/#scheduling-triggers","title":"Scheduling &amp; Triggers","text":"Job Triggers Trigger Type Configuration Usage Notes Scheduled Cron expression or simple schedule Time-based execution Supports timezone configuration File Arrival Cloud storage path monitoring Event-driven workflows Use with Auto Loader Manual No schedule On-demand execution Via UI or API Continuous Continuous execution Real-time processing Use with streaming jobs Triggered Via API call External orchestration Integrate with Airflow, etc."},{"location":"5-data-orchestration/#key-concepts","title":"Key Concepts","text":"<ol> <li>Multi-Task Jobs: Orchestrate complex workflows with task dependencies and parallel execution</li> <li>Task Dependencies: Define execution order; Databricks automatically parallelizes independent tasks</li> <li>Parameter Passing: Use widgets for inputs and <code>taskValues</code> to pass data between tasks</li> <li>Cluster Strategies: Shared clusters for faster startup vs. isolated clusters for resource control</li> <li>Error Handling: Configure retries, timeouts, and alerts for robust pipelines</li> <li>Job Repair: Re-run only failed tasks without re-executing successful ones</li> <li>Integration: Use Jobs API for CI/CD and external orchestration tools</li> </ol>"},{"location":"5-data-orchestration/#official-documentation","title":"Official Documentation","text":"<ul> <li>Databricks Jobs and Workflows</li> <li>Task Orchestration</li> <li>Jobs API Reference</li> <li>dbutils.widgets</li> <li>Databricks Notebooks</li> </ul>"},{"location":"6-data-governance/","title":"Section 6 \u2014 Data Governance","text":"<p>Quick-reference tables for data privacy, access control, and regulatory compliance.</p>"},{"location":"6-data-governance/#delta-lake-time-travel-versioning","title":"Delta Lake Time Travel &amp; Versioning","text":"Time Travel Operations Operation SQL/Code Usage Notes Query Version <code>SELECT * FROM table@v10</code> Read specific version Use for auditing Query Timestamp <code>SELECT * FROM table TIMESTAMP AS OF '2023-01-01'</code> Read at point in time More user-friendly than version Describe History <code>DESCRIBE HISTORY table</code> View version history Shows operations, timestamps, versions Restore Version <code>RESTORE TABLE table TO VERSION AS OF 10</code> Rollback to version Useful for recovering from bad updates Restore Timestamp <code>RESTORE TABLE table TO TIMESTAMP AS OF '2023-01-01'</code> Rollback to time Alternative to version-based restore Version Comparison <code>SELECT * FROM table@v1 EXCEPT SELECT * FROM table@v2</code> Find differences Use for auditing changes Change Data Feed (CDF) Operation Code/Command Usage Notes Enable CDF <code>ALTER TABLE t SET TBLPROPERTIES (delta.enableChangeDataFeed = true)</code> Track changes Must enable before capturing changes Query Changes <code>SELECT * FROM table_changes('table', start_version)</code> Get change records Returns _change_type column Stream CDF <code>.option(\"readChangeFeed\", \"true\").option(\"startingVersion\", n)</code> Stream changes Use with readStream Change Types <code>_change_type IN ('insert', 'update_preimage', 'update_postimage', 'delete')</code> Filter operations Understand change semantics Version Range <code>table_changes('table', start_version, end_version)</code> Changes between versions Useful for incremental processing"},{"location":"6-data-governance/#data-privacy-security","title":"Data Privacy &amp; Security","text":"Dynamic Views &amp; Row-Level Security Pattern SQL Code Usage Notes Redaction <code>CASE WHEN is_member('admins') THEN email ELSE 'REDACTED' END</code> Hide sensitive data Based on user group membership Role-Based Views <code>CREATE VIEW pii_view AS SELECT * WHERE is_member('pii_access')</code> Restrict row access Grant on view, not base table Conditional Filtering <code>WHERE CASE WHEN is_member('admins') THEN TRUE ELSE country = 'US' END</code> Dynamic row filtering Different users see different rows Column Masking Multiple CASE statements per column Mask multiple columns Use for PII fields Audit Logging Track view access via query history Compliance requirement Monitor sensitive data access Access Control Functions Function SQL Usage Notes is_member() <code>is_member('group_name')</code> Check group membership Returns boolean current_user() <code>current_user()</code> Get current username Use for user-specific filtering session_user() <code>session_user()</code> Get session user Similar to current_user()"},{"location":"6-data-governance/#propagating-deletes","title":"Propagating Deletes","text":"Delete Propagation Pattern Step Code/Command Usage Notes Capture Delete Requests Stream delete events to <code>delete_requests</code> table Track deletion timeline Include deadline and status Process Deletes <code>DELETE FROM table WHERE id IN (SELECT id FROM delete_requests)</code> Execute deletion Usually after grace period Enable CDF on Source <code>ALTER TABLE source SET TBLPROPERTIES (delta.enableChangeDataFeed = true)</code> Track source deletes Required for downstream propagation Stream CDF Deletes <code>.option(\"readChangeFeed\", \"true\").filter(\"_change_type = 'delete'\")</code> Capture delete operations Read from source CDF Cascade to Related Tables <code>DELETE FROM child WHERE parent_id IN (SELECT id FROM deletes_view)</code> Referential integrity Use foreachBatch pattern Update Delete Status <code>MERGE INTO delete_requests ... WHEN MATCHED THEN UPDATE SET status = 'deleted'</code> Track completion Audit trail Delete Request Management Operation Code/Pattern Usage Notes Add Deadline <code>date_add(request_timestamp, 30)</code> Compliance grace period E.g., 30-day GDPR requirement Status Tracking Status: 'requested', 'deleted', 'expired' Lifecycle management Track delete progression Verification Compare versions with EXCEPT Confirm deletions <code>table@v_before EXCEPT table@v_after</code> Batch Delete Stream <code>foreachBatch(process_deletes)</code> Custom deletion logic Access full SQL in micro-batch"},{"location":"6-data-governance/#compliance-auditing","title":"Compliance &amp; Auditing","text":"Compliance Patterns Pattern Implementation Usage Notes GDPR Right to Erasure Delete propagation with CDF 30-day deletion requirement Track request and completion dates Data Retention Time travel + vacuum policy Keep versions for audit period Balance storage cost vs. retention needs Audit Trail DESCRIBE HISTORY + CDF Track all data changes Immutable change log Access Logs Query history + dynamic views Monitor PII access Compliance reporting Data Lineage Unity Catalog lineage Track data flow Understand upstream/downstream impact Column-Level Security Dynamic views with masking Restrict column access Different views for different roles Table Properties for Governance Property Configuration Usage Notes Change Data Feed <code>delta.enableChangeDataFeed = true</code> Track all changes Required for CDC patterns Data Retention <code>delta.deletedFileRetentionDuration = \"interval 30 days\"</code> Time travel window Default is 7 days Log Retention <code>delta.logRetentionDuration = \"interval 30 days\"</code> Transaction log history Separate from data retention Column Mapping <code>delta.columnMapping.mode = 'name'</code> Support column renames Enable before renaming"},{"location":"6-data-governance/#key-concepts","title":"Key Concepts","text":"<ol> <li>Dynamic Views: Use <code>is_member()</code> to create role-based data access without duplicating tables</li> <li>Delete Propagation: Capture deletes via CDF and cascade to related tables using foreachBatch</li> <li>Time Travel: Query historical versions for auditing and rollback; configure retention periods</li> <li>Change Data Feed: Track all table changes (insert/update/delete) for compliance and downstream propagation</li> <li>Row-Level Security: Filter rows dynamically based on user attributes and group membership</li> <li>Compliance Windows: Use deadline columns to track regulatory timelines (e.g., 30-day GDPR deletion)</li> <li>Audit Trail: Leverage DESCRIBE HISTORY and CDF for immutable change logs</li> </ol>"},{"location":"6-data-governance/#official-documentation","title":"Official Documentation","text":"<ul> <li>Delta Lake Time Travel</li> <li>Delta Lake Change Data Feed</li> <li>Unity Catalog Row and Column Filters</li> <li>Delta Lake Table Properties</li> <li>Data Governance with Unity Catalog</li> <li>GDPR Compliance Patterns</li> </ul>"},{"location":"7-dlt-pipelines/","title":"Section 7 \u2014 Delta Live Tables (DLT)","text":"<p>Quick-reference tables for DLT pipelines, data quality, and incremental processing.</p>"},{"location":"7-dlt-pipelines/#dlt-table-definitions","title":"DLT Table Definitions","text":"Table Decorators &amp; Types Decorator Code/Usage Purpose Notes @dp.table <code>@dp.table def table_name()</code> Define DLT table Creates managed Delta table @dp.temporary_view <code>@dp.temporary_view def view_name()</code> Define temporary view Not persisted to storage @dp.materialized_view <code>@dp.materialized_view def view_name()</code> Define materialized view Batch-only, fully refreshed Streaming Table <code>dp.create_streaming_table(\"name\")</code> Explicitly create streaming table For use with apply_changes Named Table <code>@dp.table(name=\"custom_name\")</code> Specify table name Override function name Table Properties Property Configuration Usage Notes Partitioning <code>@dp.table(partition_cols=[\"year\", \"month\"])</code> Optimize queries Common for time-series data Append-Only <code>table_properties={\"delta.appendOnly\": \"true\"}</code> Prevent updates/deletes Faster writes, better for logs Reset Protection <code>table_properties={\"pipelines.reset.allowed\": \"false\"}</code> Prevent data loss Production safety Quality Tag <code>@dp.table(table_properties={\"quality\": \"silver\"})</code> Metadata tagging Organize data quality tiers Comment <code>@dp.table(comment=\"Description\")</code> Documentation Appears in table metadata"},{"location":"7-dlt-pipelines/#data-quality-expectations","title":"Data Quality &amp; Expectations","text":"Expectation Types Expectation Code Behavior Usage expect <code>@dp.expect(\"rule_name\", \"constraint\")</code> Record violation, continue Track quality metrics without blocking expect_or_drop <code>@dp.expect_or_drop(\"rule_name\", \"constraint\")</code> Drop invalid records Filter out bad data expect_or_fail <code>@dp.expect_or_fail(\"rule_name\", \"constraint\")</code> Stop pipeline on violation Critical data quality requirements expect_all <code>@dp.expect_all(rules_dict)</code> Apply multiple rules Pass dictionary of rule_name: constraint expect_all_or_drop <code>@dp.expect_all_or_drop(rules_dict)</code> Drop if any rule fails Strict multi-rule filtering expect_all_or_fail <code>@dp.expect_all_or_fail(rules_dict)</code> Stop if any rule fails Critical multi-rule validation Quarantine Pattern Step Code Usage Notes Define Rules <code>rules = {\"valid_price\": \"price &gt; 0\", \"valid_id\": \"id IS NOT NULL\"}</code> Validation criteria Dictionary of constraints Valid Records <code>@dp.expect_all(rules) def silver_table()</code> Pass all rules Main data flow Quarantine Table <code>@dp.expect_all_or_drop({\"quarantine\": \"NOT(...)\"}) def quarantine()</code> Inverse logic Capture failed records Shared Source Both tables use same source function Reuse transformation Define once, apply twice is_quarantined Column <code>.withColumn(\"is_quarantined\", F.expr(quarantine_rules))</code> Flag violations Include in quarantine table"},{"location":"7-dlt-pipelines/#auto-loader-cloud-files","title":"Auto Loader (Cloud Files)","text":"Auto Loader Configuration Option Code/Value Usage Notes Format <code>.format(\"cloudFiles\")</code> Enable Auto Loader For incremental file ingestion File Format <code>.option(\"cloudFiles.format\", \"json\")</code> Source file type json, csv, parquet, etc. Schema Inference <code>.option(\"cloudFiles.schemaLocation\", path)</code> Track schema evolution Stores inferred schemas Explicit Schema <code>.schema(schema_string)</code> Provide schema More efficient than inference Schema Evolution <code>.option(\"cloudFiles.schemaHints\", \"col STRING\")</code> Guide inference Override specific columns Path Globbing <code>.load(f\"{path}/year=*/month=*\")</code> Filter files by pattern Partition filtering"},{"location":"7-dlt-pipelines/#change-data-capture-cdc-in-dlt","title":"Change Data Capture (CDC) in DLT","text":"apply_changes Function Parameter Configuration Usage Notes target <code>target=\"silver_table\"</code> Destination table name Must be streaming table source <code>source=\"bronze_cdc_view\"</code> Source view/table Contains CDC records keys <code>keys=[\"customer_id\"]</code> Primary key columns Identifies unique records sequence_by <code>sequence_by=F.col(\"timestamp\")</code> Ordering column Determines which update is latest except_column_list <code>except_column_list=[\"row_status\"]</code> Exclude columns Don't write metadata columns apply_as_deletes <code>apply_as_deletes=F.expr(\"row_status = 'delete'\")</code> Delete condition When to remove records stored_as_scd_type <code>stored_as_scd_type=2</code> SCD Type 2 tracking Keep history (default is Type 1) CDC Patterns Pattern Code Usage Notes Type 1 (Latest) Default <code>apply_changes</code> Overwrite with latest No history Type 2 (History) <code>stored_as_scd_type=2</code> Track changes over time Adds <code>__START_AT</code>, <code>__END_AT</code> columns Soft Deletes <code>apply_as_deletes=F.expr(\"row_status = 'delete'\")</code> Remove deleted records Based on CDC flag Filter Before CDC <code>.filter(F.col(\"row_status\") != 'delete')</code> in source Exclude deletes If deletes not needed"},{"location":"7-dlt-pipelines/#pipeline-configuration","title":"Pipeline Configuration","text":"Development vs Production Mode Configuration Usage Notes Development Development mode in UI Fast iteration Schema inference, no optimization Production Production mode Performance optimized Required for continuous updates Triggered Triggered pipeline Run once Process available data then stop Continuous Continuous pipeline Always running Low-latency streaming Target Database Specify in pipeline settings Output location All tables created here Storage Location Custom or default Checkpoint and data storage Separate from metastore DLT API &amp; Control Operation API/CLI Usage Notes Create Pipeline <code>databricks pipelines create</code> Define new pipeline JSON configuration Update Pipeline <code>databricks pipelines update</code> Modify existing pipeline Change libraries, settings Start Pipeline <code>databricks pipelines start</code> Trigger execution Manual or scheduled Stop Pipeline <code>databricks pipelines stop</code> Halt continuous pipeline Graceful shutdown Reset Pipeline Reset in UI Clear checkpoints &amp; data Start fresh Get Status <code>databricks pipelines get</code> Check pipeline state Running, stopped, failed"},{"location":"7-dlt-pipelines/#monitoring-troubleshooting","title":"Monitoring &amp; Troubleshooting","text":"Pipeline Observability Feature Location Usage Notes Data Flow DAG DLT UI &gt; Graph Visualize dependencies See table relationships Data Quality Metrics DLT UI &gt; Data Quality View expectation results Pass/fail counts per rule Event Log System table: <code>&lt;pipeline_name&gt;_event_log</code> Detailed pipeline events Query for analysis Lineage DLT UI &gt; Lineage Track data flow Upstream/downstream tables Update History DLT UI &gt; Updates Past execution results Duration, records processed"},{"location":"7-dlt-pipelines/#key-concepts","title":"Key Concepts","text":"<ol> <li>Declarative ETL: Define desired end state; DLT manages execution order and dependencies</li> <li>Data Quality: Expectations track, filter, or block bad data with clear semantics</li> <li>Auto Loader: Incrementally ingest files from cloud storage with schema evolution support</li> <li>apply_changes: Automatic CDC processing with Type 1 or Type 2 SCD support</li> <li>Streaming Tables: Append-only incremental processing; use for real-time pipelines</li> <li>Materialized Views: Batch-processed, fully refreshed; use for aggregations</li> <li>Quarantine Pattern: Separate tables for valid and invalid records using inverse expectations</li> <li>Development Mode: Fast iteration with schema inference; switch to Production for performance</li> </ol>"},{"location":"7-dlt-pipelines/#official-documentation","title":"Official Documentation","text":"<ul> <li>Delta Live Tables Documentation</li> <li>DLT Expectations (Data Quality)</li> <li>Auto Loader with DLT</li> <li>DLT CDC with apply_changes</li> <li>DLT Python API Reference</li> <li>DLT Pipeline Configuration</li> </ul>"},{"location":"8-testing-deployment/","title":"Section 8 \u2014 Testing &amp; Deployment","text":"<p>Quick-reference tables for code organization, testing, CI/CD, and deployment patterns.</p>"},{"location":"8-testing-deployment/#code-organization-imports","title":"Code Organization &amp; Imports","text":"Notebook Imports Method Code/Command Usage Notes %run <code>%run ./path/to/notebook</code> Include notebook Shares namespace; relative or absolute path %run Parent Directory <code>%run ../Includes/helper</code> Run notebook up one level Use <code>..</code> for parent directory Python Import <code>from module import Class</code> Import Python modules Requires module in Python path Sys Path Append <code>sys.path.append(os.path.abspath('../modules'))</code> Add module directory Makes modules importable %pip install <code>%pip install package</code> or <code>%pip install ../path/to/wheel.whl</code> Install package Cluster-scoped install Module Structure Pattern Structure Usage Notes Package <code>module/__init__.py</code> Define Python package Empty or with imports Subpackage <code>module/subpkg/__init__.py</code> Nested packages Organize related modules Relative Imports <code>from .subpkg import Class</code> Within package Use dot notation Absolute Imports <code>from module.subpkg import Class</code> From anywhere After adding to sys.path Wheel Distribution <code>.whl</code> file Distribute code Build with <code>setup.py</code> Workspace Files Repos or workspace files Shared code Use %run or add to path"},{"location":"8-testing-deployment/#databricks-cli-secrets","title":"Databricks CLI &amp; Secrets","text":"Databricks CLI Commands Command Usage Purpose Notes configure <code>databricks configure --token</code> Set up authentication Creates <code>.databrickscfg</code> workspace <code>databricks workspace ls /path</code> List workspace items View notebooks, folders fs <code>databricks fs ls dbfs:/path</code> List DBFS files Browse file storage jobs <code>databricks jobs list</code> List jobs View job configurations clusters <code>databricks clusters list</code> List clusters Get cluster IDs secrets <code>databricks secrets list-scopes</code> List secret scopes View available scopes Secrets Management Operation Code/Command Usage Notes List Scopes <code>databricks secrets list-scopes</code> View all scopes CLI command Create Scope <code>databricks secrets create-scope --scope &lt;name&gt;</code> Create new scope Backend: Azure Key Vault or Databricks-managed Put Secret <code>databricks secrets put --scope &lt;scope&gt; --key &lt;key&gt;</code> Add secret Opens editor for value Get Secret <code>dbutils.secrets.get(\"&lt;scope&gt;\", \"&lt;key&gt;\")</code> Retrieve secret in code Returns string List Secrets <code>databricks secrets list --scope &lt;scope&gt;</code> View keys in scope CLI command ACL <code>databricks secrets put-acl --scope &lt;scope&gt; --principal &lt;user&gt;</code> Control access MANAGE, WRITE, READ"},{"location":"8-testing-deployment/#testing-patterns","title":"Testing Patterns","text":"Unit Testing Approaches Pattern Code/Implementation Usage Notes Assert <code>assert actual == expected, \"Error message\"</code> Simple validation Inline in notebooks Count Validation <code>assert df.count() == expected_count</code> Check row count Common after transformations Distinct Validation <code>assert df.select(\"id\").distinct().count() == total</code> Check uniqueness Validate primary keys Schema Validation <code>assert df.schema == expected_schema</code> Check structure Validate column names/types Data Quality <code>assert df.filter(\"col IS NULL\").count() == 0</code> Check constraints Validate business rules pytest <code>def test_function(): assert ...</code> Formal testing Use with CI/CD Integration Testing Pattern Approach Usage Notes Test Data Small synthetic dataset Validate pipeline end-to-end Store in DBFS or repo Temporary Tables Create test tables in temp database Isolated testing Clean up after tests Widget Parameters Pass test mode via widget Switch between test/prod <code>dbutils.widgets.get(\"mode\")</code> DLT Test Pipeline Separate development pipeline Test DLT changes Use development mode Job Test Run Trigger job with test parameters Validate orchestration Use separate test database"},{"location":"8-testing-deployment/#cicd-deployment","title":"CI/CD &amp; Deployment","text":"Deployment Strategies Strategy Approach Usage Notes Repos Integration Git sync via Databricks Repos Automatic sync with Git Best for notebooks Databricks CLI Script deployment via CLI Automate with scripts Good for jobs/clusters Databricks Asset Bundles <code>databricks bundle deploy</code> Modern deployment tool Recommended for new projects Terraform Infrastructure as code Manage all resources Enterprise standard Azure DevOps / GitHub Actions CI/CD pipeline integration Automated testing &amp; deployment Use with CLI or bundles Workspace API REST API for resource management Programmatic control Most flexible Environment Management Pattern Implementation Usage Notes Separate Workspaces Dev, staging, prod workspaces Complete isolation Enterprise pattern Separate Databases <code>dev_db</code>, <code>staging_db</code>, <code>prod_db</code> Logical separation Within same workspace Environment Variables Cluster-level env vars or secrets Configuration per environment Use secrets for sensitive data Widget Defaults Different defaults per environment Runtime configuration Set in job parameters Conditional Logic <code>if env == \"prod\":</code> Environment-specific code Use sparingly"},{"location":"8-testing-deployment/#job-parameterization","title":"Job Parameterization","text":"Parameter Patterns Pattern Implementation Usage Notes Widgets <code>dbutils.widgets.text(\"param\", \"default\")</code> Notebook inputs Set in job or notebook UI Job Parameters Set in job configuration Apply to all tasks Override task parameters Task Parameters Set per task in job Task-specific values Override job parameters Environment Detection <code>spark.conf.get(\"environment\")</code> Get current environment Set in cluster config Config Files JSON/YAML in DBFS or repos External configuration Load and parse in code Unity Catalog Volumes Store config in volumes Centralized config management New recommended approach"},{"location":"8-testing-deployment/#monitoring-logging","title":"Monitoring &amp; Logging","text":"Logging Best Practices Practice Code/Implementation Usage Notes Print Statements <code>print(f\"Processing {count} records\")</code> Simple logging Shows in driver logs Python Logging <code>import logging; logging.info(\"message\")</code> Structured logging Better for production Log Tables Write logs to Delta table Persistent audit trail Query with SQL DLT Event Log System table <code>_event_log</code> DLT-specific logging Automatic for DLT pipelines Job Run Output <code>dbutils.notebook.exit(json.dumps(result))</code> Return metrics from tasks Display in job run details Custom Metrics Write metrics to table Track KPIs over time Dashboard with SQL Error Handling Pattern Code Usage Notes Try/Except <code>try: ... except Exception as e: ...</code> Catch errors Log and handle gracefully Validation Before Processing Check conditions before execution Fail fast Better error messages Dead Letter Queue Write failed records to separate table Reprocess later Don't lose data Email Alerts Configure in job settings Notify on failure Include error details Retry Logic Job-level retry configuration Handle transient failures Set max retries Circuit Breaker Stop pipeline if too many errors Prevent cascading failures E.g., DLT expect_or_fail"},{"location":"8-testing-deployment/#key-concepts","title":"Key Concepts","text":"<ol> <li>Code Organization: Use modules and packages for reusable code; %run for notebook includes</li> <li>Module Imports: Add directories to <code>sys.path</code> or use <code>%pip install</code> for wheels</li> <li>Secrets Management: Never hardcode credentials; use Databricks secrets with scopes</li> <li>Unit Testing: Validate transformations with assertions; use pytest for formal testing</li> <li>CI/CD: Use Databricks Repos + GitHub Actions or Azure DevOps for automated deployment</li> <li>Environment Separation: Use separate workspaces or databases for dev/staging/prod</li> <li>Parameterization: Use widgets and job parameters for configurable pipelines</li> <li>Logging: Write to Delta tables for persistent audit trails; use structured logging</li> <li>Error Handling: Fail fast with validation, catch exceptions, and alert on failures</li> <li>Deployment: Databricks Asset Bundles or Terraform for infrastructure as code</li> </ol>"},{"location":"8-testing-deployment/#official-documentation","title":"Official Documentation","text":"<ul> <li>Databricks CLI</li> <li>Databricks Secrets</li> <li>Databricks Repos (Git Integration)</li> <li>Databricks Asset Bundles</li> <li>Python Modules in Databricks</li> <li>CI/CD on Databricks</li> <li>Testing on Databricks</li> </ul>"}]}