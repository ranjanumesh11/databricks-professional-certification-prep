{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"databricks-professional-certification-prep docs","text":"<p>Welcome \u2014 this minimal docs site mirrors selected README content from the repository and provides quick links.</p> <ul> <li>Section 2 \u2014 Data Modeling: https://github.com/ranjanumesh11/databricks-professional-certification-prep/blob/main/external/Databricks-Certified-Data-Engineer-Professional/2%20-%20Data%20Modeling/README.md</li> <li>Local docs page: <code>2-data-modeling.md</code> (below)</li> </ul>"},{"location":"#local-pages","title":"Local pages","text":"<ul> <li>2 \u2014 Data Modeling</li> </ul> <p>Site generated from repository content. To publish this site visit the repository Settings \u2192 Pages and select the <code>main</code> branch <code>/docs</code> folder as the source (or let the workflow deploy automatically).</p>"},{"location":"2-data-modeling/","title":"Section 2 \u2014 Data Modeling","text":"<p>Below are two concise reference sections followed by the detailed notebook explanations (theory and hands-on checklist). The first section is a high-level summary grouped by area (Databricks, Spark, Python, SQL). The second is a command/operator quick-reference table that shows syntax used in this course and a few useful extras marked with an asterisk (*).</p>"},{"location":"2-data-modeling/#1-high-level-concepts-by-area-short-bullets","title":"1) High-level concepts (by area) \u2014 short bullets","text":""},{"location":"2-data-modeling/#databricks-specific","title":"Databricks-specific","text":"<ul> <li>Autoloader (cloudFiles): incremental file ingestion for cloud storage (used to stream JSON files into Bronze). Doc: https://docs.databricks.com/data-engineering/ingestion/auto-loader/index.html</li> <li><code>availableNow</code> trigger: bounded processing of all currently-available files (demo/backfill). Doc: https://docs.databricks.com/data-engineering/ingestion/auto-loader/cloud-files-trigger.html</li> <li>Databricks Volumes / Unity Catalog: managed storage and cataloging for datasets. Doc: https://docs.databricks.com/data-governance/unity-catalog/index.html</li> <li><code>dbutils.fs</code>: workspace filesystem utilities (ls, cp, rm). Doc: https://docs.databricks.com/dev-tools/databricks-utils.html</li> </ul>"},{"location":"2-data-modeling/#spark-pyspark","title":"Spark / PySpark","text":"<ul> <li>Structured Streaming: <code>readStream</code> / <code>writeStream</code>, watermarking, triggers, <code>foreachBatch</code>. Docs: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</li> <li>DataFrame APIs: <code>from_json</code>, <code>withColumn</code>, <code>cast</code>, <code>date_format</code>, <code>dropDuplicates</code>, <code>withWatermark</code>. Docs: https://spark.apache.org/docs/latest/api/python/</li> <li>Window functions and broadcast joins for dedup/enrichment. Docs: https://spark.apache.org/docs/latest/api/python/</li> </ul>"},{"location":"2-data-modeling/#python","title":"Python","text":"<ul> <li>Core language constructs used: classes (<code>__init__</code>, <code>self</code>), lists/dicts/sets, f-strings, try/except, helper functions. Python stdlib reference: https://docs.python.org/3/library/stdtypes.html</li> </ul>"},{"location":"2-data-modeling/#sql-delta","title":"SQL / Delta","text":"<ul> <li>Delta Lake <code>MERGE</code> for upserts and Type-2 SCD patterns. Docs: https://docs.delta.io/latest/delta-update.html#merge</li> <li>Managed Delta tables via <code>.table(\"name\")</code> sink and <code>mergeSchema</code> option.</li> </ul>"},{"location":"2-data-modeling/#2-quick-commands-syntax-reference-grouped-by-area","title":"2) Quick commands &amp; syntax reference (grouped by area)","text":"<p>Notes: To improve readability this section is split into small tables per area (Databricks, PySpark, SQL/Delta, Python). Items are numbered and options or sub-settings appear as indented sub-items (e.g. <code>1.1</code>).</p> Databricks (1) \u2014 click to expand  | Item | Command / Operator | Course usage (short) | Notes &amp; extras | |---:|---|---|---| | 1 \u2014 Autoloader | `spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"json\").load(path)` | used to ingest `kafka-raw` -&gt; streaming DF | Extra options listed below (1.1, 1.2) | | 1.1 \u2014 Autoloader option | `.option(\"cloudFiles.format\",\"json\")` | specify payload format | common: `json`, `parquet`, `csv` | | 1.2 \u2014 Autoloader notifications* | `.option(\"cloudFiles.useNotifications\",\"true\")` | use S3 notifications to lower list cost | *Requires configuration of cloud events | | 2 \u2014 Trigger | `.trigger(availableNow=True)` | used in `process_bronze()` to process available files and stop | Alternative: `.trigger(processingTime='10 seconds')` | | 3 \u2014 Checkpointing | `.option(\"checkpointLocation\", checkpoint_path)` | required on writeStream for progress tracking | ensure stable storage; use unique path per query | | 4 \u2014 Partitioning (write) | `.partitionBy(\"topic\",\"year_month\")` | write partitioned Delta table | partitions improve read performance; avoid small partitions | | 5 \u2014 Delta write options | `.option(\"mergeSchema\", True).table(\"bronze\")` | allow schema evolution on sink | Extra: `.mode(\"append\")`, `format(\"delta\")` | | 6 \u2014 dbutils helpers | `dbutils.fs.ls(path)`, `dbutils.fs.cp(src,dst)`, `dbutils.fs.rm(path, True)` | copy and list dataset files (Copy-Datasets uses these) | DBFS vs `/Volumes/...` differences by catalog |  #### Short Autoloader example  <pre><code># Autoloader: read JSON files incrementally from cloud storage\nautoloader_df = (spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"json\")\n    .schema(\"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\")\n    .load(f\"{bookstore.dataset_path}/kafka-raw\")\n)\n\n# Cast value to string and parse later with from_json in downstream processing\nautoloader_df.select(\"topic\", autoloader_df.value.cast(\"string\").alias(\"value_text\")).show(5)\n</code></pre> Spark / PySpark (2) \u2014 click to expand  | Item | Command / Operator | Course usage (short) | Notes &amp; extras | |---:|---|---|---| | 2.1 \u2014 Streaming read | `spark.readStream.table(\"bronze\")` | read Bronze as streaming source | can also use `format(\"delta\")` + `.load(path)` | | 2.2 \u2014 JSON parsing | `F.from_json(F.col(\"value\").cast(\"string\"), json_schema)` | parse message payloads into struct | `from_json` returns Struct; use `.select(\"v.*\")` to expand | | 2.3 \u2014 Timestamp handling | `.withColumn(\"timestamp\", (F.col(\"timestamp\")/1000).cast(\"timestamp\"))` | convert epoch ms -&gt; timestamp | or use `to_timestamp` on string values | | 2.4 \u2014 Partition key | `F.date_format(\"timestamp\",\"yyyy-MM\")` | derive `year_month` partition column | common for time-partitioning | | 2.5 \u2014 Watermarking | `.withWatermark(\"order_timestamp\",\"30 seconds\")` | used for dedup state cleanup | choose watermark based on expected lateness | | 2.6 \u2014 Deduplication | `.dropDuplicates([\"order_id\",\"order_timestamp\"])` | remove duplicates in stream (with watermark) | `dropDuplicates` is stateful; watch memory | | 2.7 \u2014 foreachBatch | `.writeStream.foreachBatch(func).option(\"checkpointLocation\", ...).start()` | used to call upsert functions per micro-batch | `func(microBatchDF, batchId)` \u2014 use `MERGE` inside func | | 2.8 \u2014 Window &amp; rank | `Window.partitionBy(...).orderBy(F.col(...).desc())` and `F.rank()` | used in `upsert_customers_batch` for latest row selection | Window functions are powerful for dedupe &amp; SCD logic | | 2.9 \u2014 Broadcast join | `F.broadcast(df_small)` | used to enrich customers with country lookup | use for small static tables to avoid shuffle | | 2.10 \u2014 Spark actions | `df.collect()` / `df.first()` / `df.take(n)` | used to read small results to driver | Prefer `first()` / `take(1)` over `collect()` for single-row reads |   SQL / Delta (3) \u2014 click to expand  | Item | Command / Operator | Course usage (short) | Notes &amp; extras | |---:|---|---|---| | 3.1 \u2014 MERGE (Delta) | `MERGE INTO target USING source ON  WHEN MATCHED THEN UPDATE ... WHEN NOT MATCHED THEN INSERT ...` | used for upserts &amp; Type-2 SCD | Delta support for `MERGE` SQL; essential for exam |   Python (4) \u2014 click to expand  | Item | Command / Operator | Course usage (short) | Notes &amp; extras | |---:|---|---|---| | 4.1 \u2014 Basics | `class`, `__init__`, `self`, `list.append()`, `dict` | `Copy-Datasets` creates `CourseDataset` instance and calls methods | `__init__` initializes attributes; avoid heavy I/O in constructor |"},{"location":"2-data-modeling/#3-where-to-read-official-docs","title":"3) Where to read (official docs)","text":"<ul> <li>Databricks Autoloader: https://docs.databricks.com/data-engineering/ingestion/auto-loader/index.html</li> <li>Databricks <code>dbutils</code>: https://docs.databricks.com/dev-tools/databricks-utils.html</li> <li>Databricks Unity Catalog &amp; Volumes: https://docs.databricks.com/data-governance/unity-catalog/index.html</li> <li>Spark Structured Streaming guide: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</li> <li>PySpark functions (from_json, date_format, etc): https://spark.apache.org/docs/latest/api/python/</li> <li>Delta Lake MERGE &amp; update patterns: https://docs.delta.io/latest/delta-update.html#merge</li> <li>Python stdlib (core types): https://docs.python.org/3/library/stdtypes.html</li> </ul>"}]}