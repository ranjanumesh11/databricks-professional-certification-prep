{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d0d244",
   "metadata": {},
   "source": [
    "## 1) Constructor (`__init__`) and `self` (overview)\n",
    "- `__init__` is the constructor: it runs automatically when you create an instance, e.g. `bookstore = CourseDataset(...)`.\n",
    "- `self` is the conventional name for the instance inside instance methods. `self.attr` stores attributes on the instance.\n",
    "- The `CourseDataset.__init__` stores input arguments (`uri`, `data_catalog`, `db_name`, optional `location` and `checkpoint`) on the instance but does not perform heavy I/O by itself — `create_database()` and `download_dataset()` are separate methods called explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aeac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: a tiny class demonstrating __init__ and self\n",
    "class Demo:\n",
    "    def __init__(self, name, value=None):\n",
    "        # constructor runs when creating an instance\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "\n",
    "    def show(self):\n",
    "        print(f'Instance name={self.name}, value={self.value}')\n",
    "\n",
    "# create instance (constructor runs)\n",
    "d = Demo('sample', 42)\n",
    "d.show()\n",
    "\n",
    "# attributes are stored on the instance\n",
    "print('name attribute:', d.name)\n",
    "print('value attribute:', d.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec72e93",
   "metadata": {},
   "source": [
    "## 2) `@staticmethod` and `@classmethod`\n",
    "- `@staticmethod`: no `self` or `cls` is passed automatically; behaves like a regular function namespaced in the class. If you need instance state, either pass the instance explicitly or use a normal method.\n",
    "- `@classmethod`: receives the class as `cls` and is useful for alternate constructors or class-wide behavior.\n",
    "- The `Copy-Datasets` file uses `@staticmethod` for `upsert_*_batch` helpers because they operate on micro-batch DataFrames and don't require access to `self` state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b8dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of staticmethod and classmethod\n",
    "class Utils:\n",
    "    @staticmethod\n",
    "    def add(a, b):\n",
    "        return a + b\n",
    "\n",
    "    @classmethod\n",
    "    def name_of_class(cls):\n",
    "        return cls.__name__\n",
    "\n",
    "print('static add:', Utils.add(2,3))\n",
    "print('class name via classmethod:', Utils.name_of_class())\n",
    "u = Utils()\n",
    "print('call static via instance too:', u.add(5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0751b0",
   "metadata": {},
   "source": [
    "## 3) Spark: `collect()` vs `first()` / `head()` / `take()`\n",
    "- `df.collect()` returns a Python `list` of `Row` objects and brings all requested rows to the driver (expensive for large tables).\n",
    "- `df.first()` returns a single `Row` (the first row).\n",
    "- `df.head(n)` returns a `list[Row]` with up to `n` rows.\n",
    "- `df.take(n)` also returns a `list[Row]` for `n` rows.\n",
    "Use `first()` or `take(1)` for small retrievals instead of `collect()` when you only need a single value.\n",
    "Below are safe example snippets you can run in Databricks (they use `spark`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark examples (run in Databricks where `spark` is available)\n",
    "try:\n",
    "    df = spark.sql(\"SELECT current_catalog() AS catalog\")\n",
    "    # first() returns a Row object for the first row\n",
    "    row = df.first()\n",
    "    print('first() ->', row[0] if row is not None else None)\n",
    "\n",
    "    # collect() returns a list of Rows\n",
    "    all_rows = df.collect()\n",
    "    print('collect() ->', all_rows[0][0] if len(all_rows) > 0 else None)\n",
    "\n",
    "    # head(n) vs take(n)\n",
    "    small = df.head(1)   # list with up to 1 Row\n",
    "    print('head(1) ->', small)\n",
    "    small2 = df.take(1)  # list with up to 1 Row\n",
    "    print('take(1) ->', small2)\n",
    "except Exception as e:\n",
    "    print('Run this cell in Databricks where `spark` is available. Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fa8ef",
   "metadata": {},
   "source": [
    "## 4) Databricks filesystem helpers used in `Copy-Datasets`\n",
    "- `dbutils.fs.ls(path)` lists files/directories at `path`.\n",
    "- `dbutils.fs.cp(src, dst, recurse=True)` copies files between paths (DBFS, S3, Volumes).\n",
    "- `path_exists` in `Copy-Datasets` calls `dbutils.fs.ls(path)` and interprets not-found exceptions to return `False` (safer than letting the exception crash the flow).\n",
    "Run the next cell to print the `bookstore` paths after you `%run Includes/Copy-Datasets` in your notebook session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you run `%run ../Includes/Copy-Datasets` in your notebook session, run the following checks:\n",
    "try:\n",
    "    print('bookstore object exists? ->', 'bookstore' in globals())\n",
    "    if 'bookstore' in globals():\n",
    "        print('catalog_name:', bookstore.catalog_name)\n",
    "        print('db_name:', bookstore.db_name)\n",
    "        print('dataset_path:', bookstore.dataset_path)\n",
    "        print('checkpoint_path:', bookstore.checkpoint_path)\n",
    "        # list a few files if dataset_path is set\n",
    "        if bookstore.dataset_path is not None:\n",
    "            try:\n",
    "                print('Listing dataset root:')\n",
    "                display(dbutils.fs.ls(bookstore.dataset_path))\n",
    "            except Exception as e:\n",
    "                print('Could not list dataset_path (maybe mount/permission issue):', e)\n",
    "except Exception as e:\n",
    "    print('Run this after `%run ../Includes/Copy-Datasets`. Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9689bfd",
   "metadata": {},
   "source": [
    "## 5) Simulate streaming (Autoloader) — how the repo does it\n",
    "- The repo stores files in two folders: `kafka-streaming` (source files numbered like `01.json`) and `kafka-raw` (files Autoloader reads).\n",
    "- `bookstore.load_new_data(n)` moves `n` files from `kafka-streaming` → `kafka-raw`.\n",
    "- `bookstore.process_bronze()` runs an Autoloader streaming job to read `kafka-raw` and write to the `bronze` Delta table.\n",
    "Run the code below to simulate one message and run the bronze loader (only in Databricks where `bookstore` is defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5854b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate one incoming file and run the bronze loader (Databricks only)\n",
    "try:\n",
    "    if 'bookstore' in globals():\n",
    "        print('Loading one file into kafka-raw...')\n",
    "        bookstore.load_new_data(1)\n",
    "        print('Running bronze processing (availableNow) ...')\n",
    "        bookstore.process_bronze()\n",
    "        print('Bronze table rows:')\n",
    "        display(spark.table('bronze').limit(10))\n",
    "    else:\n",
    "        print('Please run `%run ../Includes/Copy-Datasets` first to create `bookstore`.')\n",
    "except Exception as err:\n",
    "    print('Error (run in Databricks with `bookstore` available):', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008ee15",
   "metadata": {},
   "source": [
    "## 6) Quick checklist & troubleshooting tips\n",
    "- If you get `NameError: bookstore is not defined`, re-run the include cell: `%run ../Includes/Copy-Datasets`.\n",
    "- If copying from S3 fails, check whether your workspace allows anonymous S3 access or you need to configure credentials or a mount.\n",
    "- If `dbutils.fs.ls(bookstore.dataset_path)` fails, inspect `/mnt` (mount points) and workspace permissions.\n",
    "- Use `spark.sql('SHOW SCHEMAS').show()` to verify `bookstore.db_name` was created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c6503",
   "metadata": {},
   "source": [
    "---\n",
    "If you want, I can also: \n",
    "- Add more runnable examples for `MERGE`/`foreachBatch` patterns used in the repo, or\n",
    "- Convert one of the notebooks to a standalone Python script that uses local paths instead of Databricks-specific APIs.\n",
    "Tell me which next step you'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8572f",
   "metadata": {},
   "source": [
    "## Python: Core Language & Objects (detailed)\n",
    "\n",
    "- **class**: a blueprint for objects. `class CourseDataset:` defines methods and attributes for dataset handling.\n",
    "- **instance / object**: a concrete value made from a class (e.g., `bookstore = CourseDataset(...)`).\n",
    "- **__init__ (constructor)**: special method called automatically when an instance is created; used to initialize `self.*` attributes.\n",
    "- **self**: conventional name for the parameter that refers to the instance inside instance methods. Not a keyword but always the first parameter for regular instance methods.\n",
    "- **method**: a function defined inside a class (instance methods take `self`).\n",
    "- **@staticmethod / @classmethod / @property**:\n",
    "  - `@staticmethod`: function in class namespace that does NOT receive `self` automatically (useful for helpers not needing instance state).\n",
    "  - `@classmethod`: receives `cls` (the class) automatically; useful for alternative constructors or class-level behavior.\n",
    "  - `@property`: exposes a method like an attribute (`obj.prop` instead of `obj.prop()`).\n",
    "- **function**: a reusable block of code defined by `def`.\n",
    "- **try / except**: exception handling; `except` can inspect or re-raise exceptions.\n",
    "- **return value**: functions may return a value; `None` is returned implicitly if no `return`.\n",
    "- **positional / keyword args**: function parameters passed by position or by name; default values shown as `param=None`.\n",
    "- **list**: ordered, mutable collection (`[1,2,3]`); methods: `.append(x)` (mutates list), `.pop()`, indexing `a[0]`.\n",
    "- **tuple**: ordered, immutable collection: `(1,2)`.\n",
    "- **dict**: key→value mapping: `{'k': v}`.\n",
    "- **set**: unordered collection of unique items.\n",
    "- **max()**: builtin that returns max value from iterable.\n",
    "- **string methods**: `.zfill(n)` pads numeric strings with leading zeros (used to build `01.json`).\n",
    "- **f-strings**: `f\"{var}/path\"` — formatted string literal (fast string interpolation).\n",
    "- **list of Rows**: when you call `df.collect()` you get a Python list of `Row` objects.\n",
    "\n",
    "## Spark / PySpark basics\n",
    "\n",
    "- **spark**: the SparkSession object used to run Spark SQL/DataFrame operations.\n",
    "- **DataFrame**: distributed table-like collection of rows and columns. Operations are lazy until an action runs.\n",
    "- **spark.sql()**: submit SQL query and return a DataFrame.\n",
    "- **DataFrame.collect()**: action that executes the query and returns a Python `list[Row]` with results (brings data to driver — can be expensive).\n",
    "- **DataFrame.first() / head()**: action returning the first Row (or `head(n)` returns a `list[Row]` of first `n` rows).\n",
    "- **DataFrame.take(n)**: returns up to `n` rows as a `list[Row]`.\n",
    "- **Row**: a record object returned by DataFrame actions; accessible by index (`row[0]`) or by name (`row['col']`).\n",
    "- **F alias** (`from pyspark.sql import functions as F`): module of Spark SQL functions used to construct expressions (e.g., `F.col`, `F.date_format`, `F.from_json`).\n",
    "- **Window**: `pyspark.sql.window.Window` used for window functions like `rank()`.\n",
    "\n",
    "## Spark SQL types and schema\n",
    "\n",
    "- **primitive types**: `STRING`, `LONG`, `BINARY`, `TIMESTAMP`, `DOUBLE`, `BIGINT`, etc. Used in schema strings.\n",
    "- **StructType / StructField**: programmatic way to define nested schemas (not used explicitly but conceptually).\n",
    "- **ArrayType, MapType**: Spark column types for arrays and maps (used when DataFrame values are nested).\n",
    "\n",
    "## Databricks-specific tools & filesystem\n",
    "\n",
    "- **dbutils**: Databricks utilities (available in notebooks). `dbutils.fs` is the file system API.\n",
    "  - `dbutils.fs.ls(path)`: list files at `path`. Returns list of `FileInfo` objects with `.name`.\n",
    "  - `dbutils.fs.cp(src, dst, recurse)`: copy files between paths (DBFS, S3, Volumes).\n",
    "  - `dbutils.fs.rm(path, recurse)`: remove files/folders.\n",
    "- **dbfs:/...**: Databricks File System scheme for unified cloud storage paths (backed by cloud object storage).\n",
    "- **/Volumes/<catalog>/<schema>/...**: Databricks Volumes (Unity Catalog volumes) path pattern used to store data when Unity Catalog is active.\n",
    "- **S3 URI (`s3://bucket/path`)**: external source object storage. Code may attempt anonymous access for public buckets.\n",
    "\n",
    "## Databricks catalog / schema / volumes\n",
    "\n",
    "- **catalog**: top-level namespace in Unity Catalog (`current_catalog()` returns active catalog). Could be `hive_metastore` on classic workspaces or a UC name.\n",
    "- **CREATE SCHEMA IF NOT EXISTS <name>**: creates a database/schema in the active catalog.\n",
    "- **CREATE VOLUME**: Databricks SQL command to create a Volume (used by Unity Catalog).\n",
    "- **mounts `/mnt/...`**: DBFS mount points to external cloud storage configured by workspace admin.\n",
    "\n",
    "## Autoloader & Structured Streaming (used by notebooks)\n",
    "\n",
    "- **Autoloader (`cloudFiles`)**: Databricks feature for incremental file ingestion. `spark.readStream.format(\"cloudFiles\")` with `.option(\"cloudFiles.format\", \"json\")` tells Autoloader to discover and stream new JSON files in a directory.\n",
    "- **spark.readStream**: starts a streaming read; returns a streaming DataFrame.\n",
    "- **.schema(schema)**: declare expected schema (recommended for streaming).\n",
    "- **.load(path)**: begin reading from the path (streaming or batch).\n",
    "- **.withColumn(...)**: add/replace a column in a DataFrame (used to convert epoch ms to timestamp and create `year_month`).\n",
    "- **cast(\"timestamp\")**: convert column type to `timestamp`.\n",
    "- **F.date_format(col, \"yyyy-MM\")**: produce year-month string for partitioning.\n",
    "- **writeStream**: streaming sink builder.\n",
    "  - **.option(\"checkpointLocation\", path)**: directory where streaming progress/checkpoints are stored — required for reliable/exactly-once semantics.\n",
    "  - **.option(\"mergeSchema\", True)**: allow schema evolution on sink Delta table.\n",
    "  - **.partitionBy(\"topic\", \"year_month\")**: write data partitioned by these columns (makes downstream reads faster).\n",
    "  - **.trigger(availableNow=True)**: Databricks trigger that processes all currently-available files as a bounded job and then stops — good for backfills/demos (not continuous streaming).\n",
    "  - **.table(\"bronze\")**: write to a managed Delta table named `bronze`.\n",
    "  - **.awaitTermination()**: block until the stream job finishes (used with `availableNow=True`).\n",
    "- **foreachBatch**: sink option to run a user function on each micro-batch DataFrame (used for merges/upserts).\n",
    "- **withWatermark**: set watermark column to allow state cleanup and windowed deduplication.\n",
    "- **dropDuplicates()**: remove duplicate rows based on specified columns (often used with watermark).\n",
    "- **broadcast(df)**: mark a small DataFrame for broadcast join to improve join performance.\n",
    "\n",
    "## Delta / SQL merge & upsert semantics\n",
    "\n",
    "- **Delta table**: transactional table format for ACID operations (writes, updates, deletes).\n",
    "- **MERGE INTO target USING source ON <cond>**: SQL statement for upsert logic — update when matched, insert when not matched.\n",
    "- **Type-2 SCD (Slowly Changing Dimension)**: pattern where updates close existing current rows and insert new rows with `current=true` (the code's `upsert_books_batch` implements this using `MERGE`).\n",
    "\n",
    "## Utility functions & implementation details in Copy-Datasets.py\n",
    "\n",
    "- **path_exists(path)**: tries `dbutils.fs.ls(path)` and interprets certain exceptions as \"not found\" (returns False) — avoids failure when a file/dir isn’t present.\n",
    "- **__get_index(dir)**: determines the next numeric index by scanning existing json files and returning the next index number (used to simulate streaming files `01.json`, `02.json`).\n",
    "- **__load_json_file / __load_data**: copy the next numbered json file(s) from a `kafka-streaming` folder to `kafka-raw` (simulate arrival of new messages).\n",
    "- **load_new_data(num_files=1)**: public helper to move `num_files` files into `kafka-raw` (simulate new events).\n",
    "- **download_dataset()**: copies dataset files from source URI to `self.dataset_path`, skipping files that already exist (idempotent).\n",
    "- **create_database()**: runs SQL to USE CATALOG, CREATE SCHEMA IF NOT EXISTS, USE SCHEMA and then calls `__configure_directories()` to set `dataset_path`/`checkpoint_path`.\n",
    "- **__configure_directories()**: sets `dataset_path` and `checkpoint_path` depending on whether the environment is `hive_metastore` (use DBFS/mounts) or Unity Catalog (create Volumes at `/Volumes/...`).\n",
    "\n",
    "## Paths & URIs\n",
    "\n",
    "- **dbfs:/...**: Databricks filesystem path (accessible across cluster nodes).\n",
    "- **/Volumes/<catalog>/<db>/...**: Unity Catalog Volume path pattern.\n",
    "- **s3://...**: Amazon S3 URI.\n",
    "\n",
    "## Common small helpers or idioms\n",
    "\n",
    "- **print(...)**: debug/info messages shown in notebook output.\n",
    "- **idempotency patterns**: checks like `path_exists` and `CREATE SCHEMA IF NOT EXISTS` are used so actions can be re-run safely.\n",
    "- **availableNow=True**: useful for demo/backfill because it processes existing files and exits (not continuous).\n",
    "\n",
    "## How this all fits together in the repo\n",
    "\n",
    "The include builds an object `bookstore` that knows:\n",
    "- Where the dataset should live (`dataset_path`) and where streaming checkpoints should be (`checkpoint_path`).\n",
    "- How to copy files from the public S3 dataset into your workspace (`download_dataset()`).\n",
    "- How to simulate streaming by moving files from `kafka-streaming` → `kafka-raw` (`load_new_data()`).\n",
    "- How to run packaged ingestion and processing pipelines (`process_bronze()`, `process_*_silver()`).\n",
    "\n",
    "The notebooks `%run ../Includes/Copy-Datasets` to get `bookstore` in their session, then call `bookstore.load_new_data()` and `bookstore.process_bronze()` to simulate streaming ingestion with Autoloader.\n",
    "\n",
    "## Collections / Data types (clarifying terms)\n",
    "\n",
    "- **Data type**: a category that determines the values an object can hold and operations allowed (e.g., `int`, `float`, `str`).\n",
    "- **Collection (container) type**: a data type that contains other objects (possibly heterogeneous). Collections implement behaviors like iteration, membership tests, indexing, etc. Examples: `list`, `tuple`, `dict`, `set`.\n",
    "- **Common built-in collections**:\n",
    "  - `list`: ordered, mutable sequence.\n",
    "  - `tuple`: ordered, immutable sequence.\n",
    "  - `dict`: mapping of keys → values, insertion-ordered since Python 3.7.\n",
    "  - `set`: unordered unique element collection.\n",
    "\n",
    "- **When to use which**: `list` for ordered mutable sequences, `tuple` for fixed records or hashable keys, `dict` for keyed lookup, `set` for uniqueness/membership.\n",
    "- **Protocols**: Iterable, Iterator, Sequence, Mapping describe the expected behaviors (methods) a collection exposes.\n",
    "\n",
    "---\n",
    "If you'd like, I can condense this into a one-page cheat-sheet or add a short exercise cell with interview-style questions and model answers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
